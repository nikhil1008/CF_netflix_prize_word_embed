{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 3:  Embeddings + Recommenders\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, April 9 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main learning objectives: (i) implement and evaluate a pre-cursor to modern word2vec embeddings; and (ii) implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw3.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Don’t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/nikhil/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'one', u'would', u'said', u'new', u'could', u'time', u'two', u'may', u'first', u'like', u'man', u'even', u'made', u'also', u'many', u'must', u'af', u'back', u'years', u'much']\n",
      "[u'one', u'would', u'said', u'new', u'could', u'time', u'two', u'may', u'first', u'like', u'man', u'even', u'made', u'also', u'many', u'must', u'af', u'back', u'years', u'much']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lower_raw_words = set(w.lower() for w in brown.words())\n",
    "raw_sents =  brown.sents()\n",
    "\n",
    "\n",
    "raw_vocab = []\n",
    "raw_vocab = lower_raw_words - stop_words \n",
    "vocab = set(raw_vocab) - set(string.punctuation)- set(['\\'\\'','``','--'])\n",
    "\n",
    "\n",
    "sents =[]\n",
    "for s in raw_sents:\n",
    "    lower_words=[]\n",
    "    for w in s:\n",
    "        if w.lower() in vocab:\n",
    "            lower_words.append(w.lower()) \n",
    "    sents.append(lower_words)\n",
    "\n",
    "#print sents[1]\n",
    "\n",
    "##########    define Target and context words ###############\n",
    "\n",
    "all_words =  [item for sublist in sents for item in sublist]\n",
    "\n",
    "fdist = nltk.FreqDist(all_words)\n",
    "\n",
    "raw_targets = fdist.most_common(5000)\n",
    "raw_context = fdist.most_common(1000) \n",
    "\n",
    "targets =[]\n",
    "for w in raw_targets:\n",
    "    targets.append(w[0])\n",
    "\n",
    "context = []\n",
    "for w in raw_context:\n",
    "    context.append(w[0])\n",
    "\n",
    "\n",
    "print targets[0:20]\n",
    "print context[0:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########   calculate co-ocurrence matrix #################\n",
    "\n",
    "co_mat = np.zeros((5000,1000))\n",
    "\n",
    "for s in sents:\n",
    "    for i in range (0, len(s)): \n",
    "        targetw = s[i]\n",
    "        contextw=[]\n",
    "\n",
    "        if (i-2>=0):\n",
    "            contextw.append(s[i-2])\n",
    "        if (i-1>=0):\n",
    "            contextw.append(s[i-1])\n",
    "        if (i+1<len(s)):\n",
    "            contextw.append(s[i+1])\n",
    "        if (i+2<len(s)):\n",
    "            contextw.append(s[i+2])\n",
    "\n",
    "        #print targetw \n",
    "        #print contextw\n",
    "        \n",
    "\n",
    "        if targetw in targets:\n",
    "            for cw in contextw:\n",
    "                if cw in context:\n",
    "                    #print targets.index(targetw)\n",
    "                    #print context.index(cw)\n",
    "                    co_mat[targets.index(targetw),context.index(cw)]+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "c_word= np.sum(co_mat,1)\n",
    "c_cont = np.sum(co_mat,0)\n",
    "\n",
    "prob_cont = c_cont / np.sum(c_cont)\n",
    "\n",
    "vec=np.zeros((5000,1000))\n",
    "\n",
    "#for i in range (0,5000):\n",
    "#    for j in range (0,1000):\n",
    "#        prob_cont_given_word = co_mat[i,j]/c_word[i]\n",
    "#        vec[i,j] = prob_cont_given_word/prob_cont[j]\n",
    "#\n",
    "#\n",
    "#print co_mat[2,[0,9]]\n",
    "#print prob_cont[0:9]\n",
    "#print c_word[0:4]\n",
    "#print vec[2,[0,11]]\n",
    "\n",
    "p_ij = np.zeros((5000,1000))\n",
    "pmi_ij = np.zeros((5000,1000))\n",
    "ppmi = np.zeros((5000,1000))\n",
    "\n",
    "\n",
    "co_mat = co_mat +1   # Add 1 smoothing \n",
    "\n",
    "sum_freq = np.sum(co_mat)\n",
    "\n",
    "temp1= np.sum(co_mat,1)\n",
    "temp2= np.sum(co_mat,0) \n",
    "\n",
    "p_i = temp1.astype(float)/sum_freq       # word probability in all contexts \n",
    "p_j = temp2.astype(float)/sum_freq       # context probabilty \n",
    "\n",
    "\n",
    "for i in range (0,5000):\n",
    "    for j in range (0,1000):\n",
    "        p_ij[i,j] = co_mat[i,j]/sum_freq\n",
    "        pmi_ij[i,j] = math.log( (p_ij[i,j] /(p_i[i]*p_j[j])),2)\n",
    "        if (pmi_ij[i,j] >0):\n",
    "            ppmi[i,j]=pmi_ij[i,j]\n",
    "        else:\n",
    "            ppmi[i,j] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=100).fit(ppmi)\n",
    "bb=kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 117, 147, 189, 227, 228, 241, 290, 337, 424, 558, 561, 628, 636, 639, 661, 691, 743, 869, 889, 895, 925, 1220, 1310, 1490]\n"
     ]
    }
   ],
   "source": [
    "def init_list_of_objects(size):\n",
    "    list_of_objects = list()\n",
    "    for i in range(0,size):\n",
    "        list_of_objects.append( list() ) #different object reference each time\n",
    "    return list_of_objects\n",
    "\n",
    "clusters = init_list_of_objects(100)\n",
    "\n",
    "for i in range (0,len(bb)):\n",
    "    clusters[bb[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cluster: \n",
      "end, next, early, began, week, half, period, full, century, late, fall, earlier, appeared, summer, evening, month, spring, fiscal, afternoon, season, spent, sunday, session, november, saturday, \n",
      "\n",
      "cluster: \n",
      "man, young, woman, \n",
      "\n",
      "cluster: \n",
      "music, feeling, cold, rest, fine, suddenly, girls, french, met, hot, beautiful, apparently, man's, blood, chief, audience, obviously, hit, moving, born, poor, sun, deep, married, bit, filled, suppose, giving, americans, names, pain, spoke, touch, bright, mary, seeing, died, named, strange, begin, fresh, explained, coffee, looks, nice, watching, killed, turning, send, somehow, wine, rain, realize, clean, warm, busy, alive, sounds, angry, \n",
      "\n",
      "cluster: \n",
      "two, three, five, \n",
      "\n",
      "cluster: \n",
      "outside, view, except, beyond, inside, reached, color, opened, corner, closed, entered, leaving, key, opposite, waited, starting, shut, \n",
      "\n",
      "cluster: \n",
      "never, ever, \n",
      "\n",
      "cluster: \n",
      "final, \n",
      "\n",
      "cluster: \n",
      "little, place, home, enough, \n",
      "\n",
      "cluster: \n",
      "city, president, york, university, washington, central, england, council, \n",
      "\n",
      "cluster: \n",
      "group, interest, members, class, evidence, type, questions, member, types, stations, classes, "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "for i in range (0,10):\n",
    "    temp= clusters[i]\n",
    "    print('\\n\\ncluster: ')\n",
    "    for j in range (0,len(clusters[i])):\n",
    "        print(targets[temp[j]], end=', ')\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yes !! Similar words are present in one cluster. For example first cluster contains words which \"kind of\" represent time. Second cluster groups man and woman. One of the clusters groups two, three , five which are numbers. This relationship was expected as words appearing in similar contexts will be closer to each other in vector space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "one-\tanother first time man every \n",
      "would-\tcould might much said things \n",
      "said-\tsay know think like would \n",
      "new-\tcity york central first yankees \n",
      "could-\twould wanted might want said \n",
      "time-\tone long still way would \n",
      "two-\tthree four several five many \n",
      "may-\tmight would must also even \n",
      "first-\tone last time later second \n",
      "like-\tsaid know good could thought \n",
      "man-\twoman one little boy things \n",
      "even-\tmuch still would felt seem \n",
      "made-\tmake one still even making \n",
      "also-\tmay even well one still \n",
      "many-\ttwo several even among people \n",
      "must-\twould may might could things \n",
      "af-\tq **zg p function polynomial \n",
      "back-\taway around along home went \n",
      "years-\tdays months weeks year minutes \n",
      "much-\twould even things little better "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim=cosine_similarity(ppmi,ppmi)\n",
    "for i in range (0,5000):\n",
    "    cos_sim[i,i]=0\n",
    "sort_sim = np.argsort(cos_sim,axis=1)\n",
    "for i in range (0,20):\n",
    "    print('\\n'+targets[i], end='-\\t')\n",
    "    for j in range (0,5):\n",
    "        print(targets[sort_sim[i,5000-j-1]], end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yes, the findings do make sense. FOr example NN of \"one\" are \"another\", \"first\" etc... which definitely makes sense. Words with similar contexts should be closer to each other in vector space, and these findings corroborate the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.551765470117\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- don’t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Database:\n",
      "No of ratings = 3255352\n",
      "No of unique movies = 1821\n",
      "No of unique users = 28978\n",
      "\n",
      "Testing Database:\n",
      "No of ratings = 100478\n",
      "No of unique movies = 1701\n",
      "No of unique users = 27555\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of ratings, \n",
    "# movies and users in each of train and test sets.\n",
    "# Your Code Here...\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')   # Ignore warning generated when 'NAN' values are created. NAN values are created intentionally for unavailable ratings\n",
    "\n",
    "\n",
    "fp = open('./netflix-dataset/TrainingRatings.txt')\n",
    "lines=fp.readlines()\n",
    "fp.close()\n",
    "temp = ([line.split(',') for line in lines])\n",
    "train_movie =  [tempo[0] for tempo in temp]\n",
    "train_user =   [tempo[1] for tempo in temp]\n",
    "train_rating = [tempo[2] for tempo in temp]\n",
    "print 'Training Database:'\n",
    "print 'No of ratings = '+str(len(train_rating))\n",
    "print 'No of unique movies = '+str(len(set(train_movie)))\n",
    "print 'No of unique users = '+str(len(set(train_user)))\n",
    "\n",
    "\n",
    "fp = open('./netflix-dataset/TestingRatings.txt')\n",
    "lines=fp.readlines()\n",
    "fp.close()\n",
    "temp = ([line.split(',') for line in lines])\n",
    "test_movie =  [tempo[0] for tempo in temp]\n",
    "test_user =   [tempo[1] for tempo in temp]\n",
    "test_rating = [tempo[2] for tempo in temp]\n",
    "print '\\nTesting Database:'\n",
    "print 'No of ratings = '+str(len(test_rating))\n",
    "print 'No of unique movies = '+str(len(set(test_movie)))\n",
    "print 'No of unique users = '+str(len(set(test_user)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here....\n",
    "\n",
    "# Mapping between Movie Ids to smaller unique integers \n",
    "\n",
    "def map_ids_to_smaller_nos (listo):\n",
    "    dicto = {}\n",
    "    no=0\n",
    "    for ido in listo:\n",
    "        if str(ido) not in dicto.keys():\n",
    "            dicto[str(ido)]=no\n",
    "            no=no+1\n",
    "    return(dicto)\n",
    "\n",
    "\n",
    "\n",
    "train_movie_dicto = map_ids_to_smaller_nos(train_movie)\n",
    "train_user_dicto =  map_ids_to_smaller_nos(train_user)\n",
    "\n",
    "# Self checking loop to ensure Map function is performing correctly, ( Maximum of mapped id should be equal to No of unique Ids -1 )\n",
    "\n",
    "if (max(train_movie_dicto.values()) != len(set(train_movie))-1):\n",
    "    print 'Error in Mapping \\n'\n",
    "if (max(train_user_dicto.values()) != len(set(train_user))-1):\n",
    "    print 'Error in Mapping \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "no_movies=len(set(train_movie))\n",
    "no_users=len(set(train_user))\n",
    "\n",
    "\n",
    "V=np.zeros((no_users,no_movies))\n",
    "\n",
    "for i in range (0,len(train_movie)):\n",
    "    V[int(train_user_dicto[train_user[i]]), int(train_movie_dicto[train_movie[i]])]=train_rating[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.06382195]]\n",
      "[[ 3.02218815]]\n",
      "[[ 2.55542432]]\n",
      "[[ 2.82846739]]\n",
      "[[ 2.67426335]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-93512cb648f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#print vec_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mvi_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mvec_i1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_to_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mvec_i2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvi_bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mvec_i3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan_to_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-197-93512cb648f2>\u001b[0m in \u001b[0;36mzero_to_nan\u001b[0;34m(vec_i)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mzero_to_nan\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvec_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mvec_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvec_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Following is NAIVE IMPLEMENTATION : \n",
    "#Initially I coded using following approach, then I realized that code takes forever to execute.\n",
    "# Nevertheless, I am keeping the code intact here\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math \n",
    "import time\n",
    "def zero_to_nan (vec_i):\n",
    "    for jjj in range (0, len(vec_i)):\n",
    "        if (vec_i[jjj] == 0):\n",
    "            vec_i[jjj] = float('nan')\n",
    "    return vec_i\n",
    "\n",
    "def nan_to_zero (vec_i):\n",
    "    for jjj in range (0, len(vec_i)):\n",
    "        if (math.isnan(vec_i[jjj])):\n",
    "            vec_i[jjj] = 0\n",
    "    return vec_i\n",
    "\n",
    "\n",
    "predicted_ratings = []\n",
    "\n",
    "\n",
    "for ii in range(0,10):\n",
    "    \n",
    "    a = train_user_dicto[test_user[ii]] \n",
    "    j = train_movie_dicto[test_movie[ii]]\n",
    "    vec_a = V[a,:]\n",
    "    va_bar=np.mean(vec_a[vec_a!=0])\n",
    "    vec_a1 = zero_to_nan(vec_a)\n",
    "    vec_a2 = vec_a - va_bar\n",
    "    vec_a3 = np.nan_to_num(vec_a)\n",
    "    p_aj = 0\n",
    "    sum_w = 0\n",
    "    for i in range (0, V.shape[0]):\n",
    "        if ((i != a) & (V[i,j] !=0)):\n",
    "            vec_i = V[i,:]\n",
    "            #print vec_i\n",
    "            \n",
    "            start_time = time.time()\n",
    "            vi_bar = np.mean(vec_i[vec_i!=0])\n",
    "            print(\"--- %s mean ---\" % (time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "            vec_i1=zero_to_nan(vec_i)\n",
    "            print(\"--- %s zero2nan ---\" % (time.time() - start_time))\n",
    "            \n",
    "            vec_i2 = vec_i - vi_bar\n",
    "            vec_i3=np.nan_to_num(vec_i)\n",
    "            w_ai=cosine_similarity(np.reshape(vec_a3,[1,-1]),np.reshape(vec_i3,[1,-1]))\n",
    "            #print w_ai,k * w_ai * (V[i,j]-vi_bar)\n",
    "            p_aj +=  w_ai * (V[i,j]-vi_bar)\n",
    "            sum_w += w_ai\n",
    "    p_aj = p_aj / sum_w      # Normalization - K factor (K CAN NOT be 0.1 - it should be used to maintain sum(w_ai) =1)\n",
    "    p_aj = p_aj + va_bar\n",
    "    print p_aj\n",
    "    predicted_ratings.append(p_aj)\n",
    "print predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45098039 -1.45098039  0.         ...,  0.          0.          0.        ]\n",
      "(28978, 28978)\n"
     ]
    }
   ],
   "source": [
    "####  SMART  IMPLEMENTATION ###############\n",
    "\n",
    "# I have converted everything into matrix form: \n",
    "# Basically the most important term in both equations is V(i,j) - Vi_mean  \n",
    "# To compute this, I converted original V[i,j] matrix into mean substracted matrix.\n",
    "# Trick here is to convert unknown ratings into \"nan\" so that those are not involved in mean computation (np.nanmean) and\n",
    "#then convert them to zero so that their contribution in predicting the ratings becomes zero.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "M = V\n",
    "M[M == 0] = np.nan\n",
    "M_bar=np.nanmean(M,1)\n",
    "M=np.add(M, np.reshape(-M_bar,[M.shape[0],-1]))\n",
    "M=np.nan_to_num(M)\n",
    "w = cosine_similarity(M,M)\n",
    "print M[0,:]\n",
    "print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28978, 1821)\n"
     ]
    }
   ],
   "source": [
    "print M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_test=[]\n",
    "vnorm_test =[]\n",
    "p_bar_test =[]\n",
    "matching =[]\n",
    "for ii in range(0,len(test_user)):    \n",
    "#for ii in range(0,10):    \n",
    "    a = train_user_dicto[test_user[ii]] \n",
    "    j = train_movie_dicto[test_movie[ii]]\n",
    "    w_test.append(w[a,:])\n",
    "    p_bar_test.append(M_bar[a])\n",
    "    vnorm_test.append(M[:,j])\n",
    "    matching.append(a)\n",
    "\n",
    "w_test=np.array(w_test)\n",
    "vnorm_test=np.array(vnorm_test)\n",
    "#vnorm_test=np.transpose(vnorm_test)\n",
    "p_bar_test=np.array(p_bar_test)\n",
    "\n",
    "var =[] \n",
    "for i in range (0,w_test.shape[0]):\n",
    "    var.append(np.sum(np.multiply(w_test[i,:],vnorm_test[i,:])))\n",
    "var=np.array(var)\n",
    "\n",
    "w_sum=np.sum(w_test,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_norm=np.divide(var,w_sum)       # k is inverse of sum of all weights waj over all j \n",
    "var_norm[np.isnan(var_norm)] = 0\n",
    "predict_ratings = np.add(p_bar_test,var_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.772049309323\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "true_ratings=[]\n",
    "for r in test_rating: \n",
    "    true_ratings.append(float(r.translate(None, '\\t\\n ')))\n",
    "true_ratings=np.array(true_ratings)\n",
    "\n",
    "print 'Mean Absolute Error : '+str(mean_absolute_error(true_ratings, predict_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error = 2.87543251544\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(true_ratings, predict_ratings))\n",
    "print 'RMS error = '+str(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dh 80\n",
      "__ 37\n",
      "test_user 824472\n",
      "train_user_dicto 3146008\n",
      "__builtin__ 56\n",
      "no_users 24\n",
      "cosine_similarity 120\n",
      "line 55\n",
      "quit 64\n",
      "_i7 322\n",
      "_i6 206\n",
      "_i5 186\n",
      "_i4 1436\n",
      "_i3 566\n",
      "_i2 1472\n",
      "_i1 2464\n",
      "__package__ 16\n",
      "exit 64\n",
      "get_ipython 80\n",
      "_i 206\n",
      "np 56\n",
      "__doc__ 101\n",
      "fp 144\n",
      "test_movie 824472\n",
      "map_ids_to_smaller_nos 120\n",
      "__builtins__ 56\n",
      "M 422151616\n",
      "M_bar 231920\n",
      "_ih 136\n",
      "tempo 168\n",
      "sys 56\n",
      "var 40\n",
      "V 422151616\n",
      "__name__ 45\n",
      "___ 37\n",
      "_ 37\n",
      "_sh 56\n",
      "obj 280\n",
      "test_rating 824472\n",
      "i 24\n",
      "no_movies 24\n",
      "_iii 1436\n",
      "_ii 186\n",
      "In 136\n",
      "train_movie_dicto 196888\n",
      "_oh 280\n",
      "Out 280\n"
     ]
    }
   ],
   "source": [
    "# Getting the size of variables and deleting unwanted variables to avoid kernel crash\n",
    "\n",
    "import sys\n",
    "del train_user, train_movie, train_rating, temp, lines, w\n",
    "for var, obj in locals().items():\n",
    "    print var, sys.getsizeof(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving additional variables to file . In case Kernel crashes. \n",
    "\n",
    "import pickle \n",
    "with open('objs1.pkl', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([ M, M_bar], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The problem with \"neighborhood\" approach that I implemented above is it doesn’t scale particularly well to massive datasets. The key concern is that ratings matrices may overfit and noisy representations of user tastes and preferences. When we use distance based “neighborhood” approaches on raw data, we match on sparse, low-level details that we assume represent the user’s preference vectors instead of matching on the vectors themselves. It’s a subtle difference, but it’s important.\n",
    "\n",
    "For example, if I’ve listened to ten Pink Floyd songs and you’ve listened to ten different Pink Floyd songs, the raw user action matrix wouldn’t have any overlap. Mathematically, the dot product of our action vectors would be 0. We’d be in entirely separate neighborhoods, even though it seems pretty likely we share at least some underlying preferences.\n",
    "\n",
    "To resolve this, I need a method that can derive tastes and preference vectors from the raw data.\n",
    "\n",
    "And the best way to do this would be to decompose user-item matrix using SVD. Following is the implementation and I was able to achieve RMSE of 0.96 using SVD with k = 200 ( k is again a hyperparameter and can be experimented with to increase the accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(M, k = 200)\n",
    "s_diag_matrix=np.diag(s)\n",
    "all_mat = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_bar=np.reshape(M_bar,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_mat = all_mat + M_bar\n",
    "predict_ratings=[]\n",
    "for ii in range(0,len(test_user)):    \n",
    "    a = train_user_dicto[test_user[ii]] \n",
    "    j = train_movie_dicto[test_movie[ii]]\n",
    "    predict_ratings.append(all_mat[a,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.771227744546\n",
      "RMS error = 0.968799625802\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "true_ratings=[]\n",
    "for r in test_rating: \n",
    "    true_ratings.append(float(r.translate(None, '\\t\\n ')))\n",
    "true_ratings=np.array(true_ratings)\n",
    "\n",
    "print 'Mean Absolute Error : '+str(mean_absolute_error(true_ratings, predict_ratings))\n",
    "\n",
    "# Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(true_ratings, predict_ratings))\n",
    "print 'RMS error = '+str(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert discussion here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
